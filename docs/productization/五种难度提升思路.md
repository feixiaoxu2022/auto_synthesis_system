# 如何设计高难度且可验证的Agent评测任务

## 核心挑战

当前Agent评测面临的核心矛盾：**如何在保证可验证性的前提下，设计出高难度的评测任务？**

关键约束：

- ✅ **可验证**：必须有客观标准，能达成共识，不依赖LLM-as-Judge
- ✅ **高难度**：能有效区分不同模型的能力水平（30-50轮tool call）
- ✅ **真实性**：源自真实场景，不是为测试而测试

## 五种设计方法

基于公开benchmark的成功实践，总结出五种可行的难度提升方法：

---

## 方法1: 通过复杂业务规则提升难度

### 设计原理

在system prompt中设计大量条件分支和约束规则，通过final_state验证业务规则是否被正确执行。

**复杂性体现在**：**system prompt** - 包含大量条件分支、约束规则和业务逻辑

### 设计要点

1. **规则要足够复杂**：至少3-5个条件分支
2. **前置依赖要清晰**：必须按顺序获取信息
3. **状态可验证**：通过final_state检查最终结果
4. **约束要明确**：什么情况下用什么策略

### 实际案例：Tau-Bench零售换货场景

```
用户需求："我收到订单#W2378156，想把机械键盘换成clicky开关的，
          智能温控器换成Google Home兼容的。
          如果没有clicky+RGB+全尺寸的键盘，就不要背光的也行。"

设计的复杂规则：
  规则1: 必须先验证用户身份（find_user_id通过姓名+邮编）
  规则2: 必须获取订单详情（get_order_details）
  规则3: 必须查询产品规格（get_product_details）
  规则4: 必须判断替换产品是否满足条件（clicky开关/Google Home）
  规则5: 如果第一选择不可用，执行降级逻辑（放弃RGB背光）
  规则6: 最终必须创建换货订单

验证方式：
  - 检查工具调用序列是否完整
  - 检查条件分支是否正确执行
  - 检查最终状态是否符合业务规则
```

### 适用场景

订单处理、客服对话、数据筛选、工作流自动化

---

## 方法2: 通过领域知识门槛提升难度

### 设计原理

在query中嵌入需要专业领域知识才能完成的任务，答案唯一但需要深厚知识储备。

**复杂性体现在**：**query** - 需要特定技术栈、系统知识或专业领域的理解

### 设计要点

1. **知识门槛要高**：需要专业技能或系统知识
2. **答案唯一明确**：可通过系统状态验证
3. **难度可分级**：简单→进阶→高难度

### 实际案例：Terminal-Bench系统操作场景

```
简单任务：
  Query: "Create a file called /app/hello.txt. Write 'Hello, world!' to it."
  验证: 检查文件是否存在 + 内容是否正确
  知识要求: 基础文件操作

进阶任务：
  Query: "Find all Python files in the repo that import 'numpy'
         but don't have type hints. Output to /app/result.txt"
  验证: 检查输出文件内容
  知识要求:
    - 知道find/grep命令组合
    - 理解Python的import语法
    - 理解type hints的语法特征

高难度任务：
  Query: "Set up a Flask app with Redis caching and deploy to port 8080.
         The /health endpoint should return Redis connection status."
  验证:
    - HTTP请求/health返回200
    - Redis连接状态正确
  知识要求: Flask/Redis/Docker配置
```

### 适用场景

系统操作、代码开发、技术配置

### 局限性

- ⚠️ 领域知识往往能通过Web搜索获得
- ⚠️ 不适合通用生产力任务

---

## 方法3: 设计没有标准答案但有人类共识的任务

### 设计原理

选择那些**没有客观标准答案，但人类会有较高共识**的判断类任务。虽然没有标准答案，但**同样是rule-based精准验证**，只是这个rule基于大多数人的共识（如≥70%同意）。

**复杂性体现在**：**任务选择** - 选择那些能考察模型能力、但无标准答案、却有人类共识的任务领域（如信息重要性判断、方案优劣评估、内容质量评价等）

### 设计要点

1. **选择合适的任务领域**：找到那些没有标准答案但人类有共识的判断类任务
2. **建立共识标准**：通过10+人类评审独立标注
3. **设定阈值**：≥70%的人认为正确才算正确
4. **确保任务难度**：不能是简单的主观题，要真正考察模型的判断能力

### 实际案例：BFCL v4 Memory任务

```
任务领域：信息重要性判断
场景设计：咖啡机购物客服对话
前置对话：用户提到了20多条信息（订单号、配件、折扣、配送问题、厨房大小等）

测试问题：
  "What is my first name?"
  "How old am I?"
  "Which part of my espresso machine arrived bent?"
  "What magazine does my kitchen now look like a page out of?"

为什么没有标准答案：
  - 不同场景下重要性不同（客服系统 vs CRM系统）
  - 个人隐私敏感度不同
  - 业务需求可能变化

为什么有人类共识：
  - 多数人认为：first name、订单异常（bent part）应该记住
  - 多数人认为：厨房像哪本杂志、包装盒破损程度不用记

验证方式：
  Step 1: 10个人类评审独立标注（哪些信息应该被memory tools记住）
  Step 2: 计算共识度（≥70%为标准）
  Step 3: Agent的选择与人类共识对比

  如果≥70%的人认为应该记住某条信息，Agent也记住了 → 正确
```

### 适用场景

信息重要性判断、方案优劣评估、内容质量评价、策略选择建议

### 其他可能的任务领域示例

**营销文案评估**：
- 任务：从3个文案中选出最有吸引力的
- 没有标准答案：创意类任务
- 有人类共识：70%的人会选同一个

**UI设计选择**：
- 任务：两个界面设计哪个用户体验更好
- 没有标准答案：审美和UX判断
- 有人类共识：多数设计师会有相似评价

**商务决策建议**：
- 任务：在当前情况下应该选A方案还是B方案
- 没有标准答案：取决于风险偏好和战略
- 有人类共识：有经验的决策者会倾向某个选择

### 局限性

- ⚠️ 需要招募真实用户标注
- ⚠️ 成本高（每个任务需要10+人）
- ⚠️ 可能被质疑"不够客观"

---

## 方法4: 通过多轮需求变更提升难度

### 设计原理

通过多轮对话，让需求不断变化。测试Agent的上下文理解和动态调整能力。最终状态可验证。

**复杂性体现在**：**多轮query/用户模拟器prompt** - 通过对话轮次增加和需求变更来累积上下文复杂度

### 设计要点

1. **渐进式披露信息**：不要一次性给完所有信息
2. **中途变更需求**：不只是渐进披露，而是推翻重来
3. **延长对话轮次**：30-50轮才能完成
4. **最终状态可验证**：验证Agent是否正确处理了所有变更

### 实际案例：改造Tau-Bench任务

```
原始设计：零售换货任务（10轮左右）

增强版设计（30-50轮）：
  Round 1-5: 正常换货流程（键盘+温控器）
  Round 7: "等等，我查了下，温控器其实不急，先别换了"
             → 测试取消操作
  Round 9: "键盘的话，我再想想，要不要upgrade到机械轴更好的"
             → 测试需求变更
  Round 11: "对了，之前那个订单能加个鼠标吗？凑单免运费"
             → 测试新增需求
  Round 15: "算了鼠标不要了，键盘就换最基础的clicky款"
             → 测试取消新增需求
  Round 17: Agent最终执行：只换一个键盘，取消温控器换货

验证方式：
  - 最终订单状态正确（只有键盘换货）
  - 温控器换货被取消
  - 没有添加鼠标
```

### 适用场景

客服对话、需求收集、项目管理

---

## 方法5: 通过信息洋葱提升难度

### 设计原理

将关键信息分散在多个地方，需要多步推理才能找到。每一步得到的信息都是下一步的线索。

**复杂性体现在**：**环境信息和tool设计** - 通过设计多层信息嵌套和长推理链，让关键信息需要逐步挖掘

### 设计要点

1. **设计推理链**：7-10步推理链，每步都不能错
2. **信息分散存储**：关键信息分散在不同来源
3. **线索式引导**：前一步的结果是后一步的输入
4. **答案唯一明确**：可自动验证

### 实际案例：GAIA多步推理任务

#### Level 1（3-4步推理）

```
问题设计：
"If Eliud Kipchoge could maintain his record-making marathon pace indefinitely,
 how many thousand hours would it take him to run the distance between Earth and Moon
 at its closest approach?"

推理链设计：
  Step 1: 搜索 → Kipchoge的马拉松世界纪录配速（信息在网上）
  Step 2: 搜索 → 月球最近距离perigee（信息在网上）
  Step 3: 计算 → 距离 ÷ 速度 = 时间
  Step 4: 转换 → 转成千小时并四舍五入

答案："17"（千小时）
难度来源：4步信息检索+计算，任何一步错都会导致最终答案错误
```

#### Level 2（5-6步推理）

```
问题设计：
"A paper about AI regulation submitted to arXiv.org in June 2022 shows
 a figure with three axes. Which of these axis label words is used to describe
 a type of society in a Physics paper submitted on August 11, 2016?"

推理链设计：
  Step 1: 搜索arXiv → 2022年6月AI regulation论文
  Step 2: 下载PDF → 找到三轴图
  Step 3: 识别图片 → 提取六个轴标签词
  Step 4: 搜索arXiv → 2016年8月11日的Physics论文
  Step 5: 读论文 → 找societal类型描述
  Step 6: 匹配 → 哪个词同时出现在两个地方

答案："egalitarian"
难度来源：跨时间、跨领域、多模态信息整合
```

#### 扩展到生产力场景

```
设计思路：
"在Anthropic工程博客2024年的prompt engineering文章中，
 哪篇文章的作者之前在Google工作？
 该作者提到的第3个核心技巧是什么？"

推理链（8步）：
  Step 1: 访问Anthropic工程博客
  Step 2: 筛选2024年文章
  Step 3: 识别prompt engineering相关文章
  Step 4: 逐篇查看作者信息
  Step 5: 搜索作者背景 → 找出谁在Google
  Step 6: 阅读该作者的文章
  Step 7: 定位"核心技巧"章节
  Step 8: 提取第3条

难度来源：8步推理链，每步都不能错
```

### 适用场景

信息检索、数据分析、多源信息整合

### 特殊变体：游戏类任务设计

游戏类评测是"信息洋葱"方法的一个重要扩展，特别适合测试Agent的**策略规划**和**长期决策**能力。

#### 与信息洋葱的关系

**相似点**：
- 都需要处理复杂的环境信息（游戏状态空间 ≈ 多层信息嵌套）
- 都需要多步推理（每步决策基于当前状态）
- 都有明确的验证标准（游戏结果客观可测）

**差异点**：
- 信息洋葱：**线性推理链**（Step 1→2→3...→最终答案）
- 游戏类：**树形搜索空间**（每步有多个分支，需要预判未来）

#### 设计原理

**复杂性来源**：
1. **状态空间大小**：多少种可能的局面
2. **决策树深度**：需要预判多少步
3. **信息不完全性**：隐藏信息、随机性、对手策略
4. **约束交互复杂度**：规则之间的相互影响

**验证方式**：
- 确定性游戏（数独、推箱子）：是否达到目标状态
- 对抗性游戏（围棋、扑克）：与标准对手的胜率/Elo分数
- 速度类游戏：达成目标的步数/时间

#### 实际案例

**NetHack Learning Environment**：
```
任务：在NetHack游戏中生存并获得高分
设计要点：
  - 状态空间巨大（地牢随机生成）
  - 需要长期规划（食物管理、装备升级）
  - 探索vs利用权衡（冒险寻宝 vs 安全推进）

验证方式：
  - 游戏分数
  - 通关率
  - 达到特定关卡的成功率
```

**Minecraft任务（MineRL）**：
```
任务：从头开始在Minecraft中挖到钻石
设计要点：
  - 视觉理解（识别环境和资源）
  - 工具链规划（木镐→石镐→铁镐→钻石）
  - 多步骤执行（砍树→合成→挖矿→探索洞穴）

验证方式：
  - 是否成功获得钻石
  - 完成任务的时间/步数
  - 资源利用效率
```

**德州扑克（Pluribus风格）**：
```
任务：与多个对手进行德州扑克对战
设计要点：
  - 不完全信息博弈（看不到对手的牌）
  - 对手建模（推断对手策略）
  - 风险决策（何时加注/弃牌/跟注）
  - 概率推理（计算胜率）

验证方式：
  - 与人类专家对战的盈利率
  - Elo评分系统
  - 多轮次比赛的稳定性
```

#### 游戏类任务的独特价值

1. **自带难度梯度**：从新手关到专家关，天然分级
2. **可重复测试**：同一关卡可以反复测试不同策略
3. **多维能力考察**：
   - 短期规划 vs 长期战略
   - 探索 vs 利用权衡
   - 不确定性决策
   - 对手策略推断（对抗类游戏）

#### 适用场景

策略规划、决策优化、多步骤任务执行、视觉理解+操作

---

## 五种方法对比

| 设计方法 | 复杂性体现在       | 难度来源   | 验证方式 | 成本 | 适用范围 |
| -------- | ------------------ | ---------- | -------- | ---- | -------- |
| 复杂规则 | system prompt      | 条件分支多 | 状态验证 | 低   | 中等     |
| 领域知识 | query              | 知识门槛高 | 答案验证 | 低   | 较小     |
| 人类共识 | 机制设计           | 共识判断   | 人工标注 | 高   | 较大     |
| 多轮变更 | 多轮query/用户模拟 | 上下文累积 | 状态验证 | 中   | 很大     |
| 信息洋葱 | 环境信息+tool设计  | 推理链长   | 答案验证 | 低   | 很大     |
| └ 游戏类 | 游戏状态空间+规则  | 搜索空间大 | 游戏结果 | 中   | 中等     |

**注**：游戏类是方法5（信息洋葱）的特殊变体，从线性推理链扩展到树形搜索空间。

---

## 推荐组合策略

### 核心集（20-25个任务）

基于成本、难度、覆盖面的综合考量，推荐组合：

- **方法5（信息洋葱）**: 10-12个 ⭐️ 主力
  - 难度高且可控（推理链长度）
  - 完全可验证
  - 天然适合生产力场景
  - **可选游戏类任务2-3个**：测试策略规划能力

- **方法4（多轮变更）**: 6-8个 ⭐️ 主力
  - 测试上下文管理（方法5测不到）
  - 真实场景（需求确实会变）
  - 可以把简单任务变复杂

- **方法1（复杂规则）**: 4-5个
  - 测试规则理解能力
  - 经典的benchmark设计方式

### 扩展集（可选）

- **方法2（领域知识）**: 2-3个
  - 适合特定领域（代码、系统）

- **方法3（人类共识）**: 3-5个（如果有预算）
  - 需要人力投入，但能覆盖创意任务

---

## 设计原则总结

### 三个必须

1. **难度要够**：30-50轮tool call，7-10步推理
2. **能验证**：不依赖LLM-as-Judge，有客观标准
3. **够真实**：源自真实用户需求，不是为测试而测试

### 五种方法不互斥

这五种方法可以组合使用：
- 在一个任务中同时用复杂规则+多轮变更
- 在信息洋葱中嵌入领域知识
- 用人类共识验证复杂规则的执行效果
- 将游戏机制融入生产力任务（如项目管理的资源分配博弈）

### 推荐起点

对于生产力场景，**方法5（信息洋葱）+ 方法4（多轮变更）** 是最适合的组合。

如果需要测试策略规划能力，可以在方法5中加入**游戏类任务**（如资源管理游戏、决策树优化问题）。
