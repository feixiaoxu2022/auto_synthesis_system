# 分析质量检查清单

## 核心10项快速检查

### 分析完整性

- [ ] 完整阅读了conversation_history所有轮次
- [ ] 检查了final_state与expected_state的差异
- [ ] 验证了user_simulator_prompt与实际行为的一致性
- [ ] 查看了相关工具的实现代码（当涉及状态异常时）
- [ ] 查看了BusinessRules中的相关规则

### 归因准确性

- [ ] 结论基于对话事实而非prompt假设
- [ ] 区分了用户实际表达vs设定期望
- [ ] 使用了三层验证机制（当final_state异常时）
- [ ] 每个结论都有明确的证据支撑
- [ ] 排除了其他可能的归因分类

---

## 分析思路完整性检查

### 步骤覆盖检查

- [ ] 第1步：提取了评测指标和失败点
- [ ] 第2步：逐轮分析了对话内容
- [ ] 第3步：对比了期望vs实际
- [ ] 第4步：推导了失败根因
- [ ] 第5步：验证了final_state
- [ ] 第6步：验证了用户模拟器行为
- [ ] 第7步：整理了关键证据
- [ ] 第8步：确定了归因结论

### 证据链完整性

- [ ] 引用了具体对话内容（标明轮次）
- [ ] 引用了JSON数据中的关键字段
- [ ] 说明了推理逻辑的每一步
- [ ] 区分了事实观察vs推理判断

---

## 归因分类自检

### Agent能力问题自检

- [ ] BusinessRules是否明确要求该操作？
- [ ] Agent是否有机会执行但未执行？
- [ ] Agent是否理解错误导致执行偏差？
- [ ] 是否排除了样本/系统问题？

### 样本设计问题自检

- [ ] Checker条件是否与BusinessRules一致？
- [ ] BusinessRules是否明确定义了该操作？
- [ ] user_simulator_prompt是否设计合理？
- [ ] expected_state是否符合业务逻辑？

### 用户模拟器执行问题自检

- [ ] prompt本身设计是否合理？
- [ ] 实际行为是否偏离了prompt？
- [ ] 偏离是否影响了评测结果？
- [ ] 是否排除了prompt设计问题？

### 系统问题自检

- [ ] 是否查看了相关工具代码？
- [ ] 工具实现是否与设计一致？
- [ ] Checker逻辑是否正确？
- [ ] 是否排除了Agent/样本问题？

---

## 常见遗漏检查

### 对话分析遗漏

- [ ] 是否注意到Agent给用户表达需求的机会？
- [ ] 是否注意到用户在某轮表达了关键需求？
- [ ] 是否注意到Agent的工具调用返回异常？
- [ ] 是否注意到多轮对话间的逻辑不一致？

### final_state分析遗漏

- [ ] 是否检查了所有异常字段？
- [ ] 是否追溯了Agent的相关操作？
- [ ] 是否验证了工具职责边界？
- [ ] 是否检查了Agent是否遗漏操作？

### 用户模拟器分析遗漏

- [ ] 是否逐项验证了prompt遵循情况？
- [ ] 是否检查了信息透露时机？
- [ ] 是否验证了STOP时机的合理性？
- [ ] 是否区分了prompt设计vs执行问题？

---

## 输出质量检查

### 分析报告完整性

- [ ] 包含基础信息（场景、样本ID、结果）
- [ ] 包含失败点列表
- [ ] 包含对话分析摘要
- [ ] 包含证据列表
- [ ] 包含归因分类和理由
- [ ] 包含改进建议

### 分析报告清晰度

- [ ] 结论表述清晰明确
- [ ] 证据引用格式规范
- [ ] 推理逻辑层次分明
- [ ] 改进建议具体可操作

---

## 自我检查问题清单

分析完成后，问自己：

1. 我是否完整阅读了所有对话内容？
2. 我的结论是基于对话事实还是基于prompt假设？
3. 我是否区分了用户实际表达的需求vs设定的期望？
4. 我是否确认了Agent给用户表达需求的机会？
5. 我是否验证了用户模拟器的行为是否严格符合其预设？
6. 我是否检查了final_state与Agent操作意图的一致性？
7. 我是否完整记录了分析思路和证据链？
8. 我的每个结论是否都有明确的事实支撑？
